import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
#IMPORTANT NOTE. we're just using a simple single input example here, so it looks very similar to what a typisch polynomial regression can also do
# The diff is that polynom regression aims to reduce the MSE, and treats every point's error the same.
# In SVR (support vector regression), it tolerates errors within a specified margin, but penalises heavily errors beyond that margin
# Different types of kernels can be used in SVR to map the data to a higher dimension, so that the features and the target var is now linearly related
x_coords = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])
y_offsets = np.array([10, 15, 25, 39, 51, 62, 76, 77, 93])

x_coords_reshaped = x_coords.reshape(-1, 1)  # reshape x_coords to be 2D (for SVR) (1d for y is fine)

# normalise
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x_coords_reshaped)

# fit SVR model
svr = SVR(kernel='rbf', C=1e3, gamma=0.11) #C=penalty on errors, gamma = influence of single point on model
svr.fit(x_scaled, y_offsets)

# generate test data
x_vals = np.linspace(10, 90, 500).reshape(-1, 1)  # generate 500 points between 10 and 90
x_vals_scaled = scaler.transform(x_vals)  # scale x values the same way

# predict  
predicted_offsets = svr.predict(x_vals_scaled)
# print(predicted_offsets)

# plotting
plt.figure(figsize=(10, 6))
plt.scatter(x_coords, y_offsets, color='red', label="Original Data Points", zorder=5)

# ideal line generated by SVR
plt.plot(x_vals, predicted_offsets, color='blue', label="SVR Prediction Line", linewidth=2)

# Labels and title
plt.xlabel('X Coordinate')
plt.ylabel('Offset')
plt.title('SVR - Fitted Line vs Original Data Points')
plt.legend()

# Show plot
plt.show()